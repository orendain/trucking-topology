# For a list of constants for all the configurations possible on a Storm cluster and Storm topology, check out:
# http://storm.apache.org/releases/current/javadocs/org/apache/storm/Config.html
#
# Most corresponding default values are available and can be seen here:
# https://github.com/apache/storm/blob/v1.0.2/conf/defaults.yaml

# Kafka config references
# https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.5.0/bk_storm-component-guide/content/storm-kafkaspout-config-core.html
# https://github.com/apache/storm/tree/master/external/storm-kafka
# https://cwiki.apache.org/confluence/display/KAFKA/FAQ

topology {

  # Specifies how many processes you want allocated around the cluster to execute the topology.
  # Each component in the topology will execute as many threads. The number of threads allocated to a given
  # component is configured through the setBolt and setSpout methods. Those threads exist within worker processes.
  # Each worker process contains within it some number of threads for some number of components.
  # For instance, you may have 300 threads specified across all your components and 50 worker processes specified
  # in your config. Each worker process will execute 6 threads, each of which of could belong to a different component.
  # You tune the performance of Storm topologies by tweaking the parallelism for each component and the number of
  # worker processes those threads should run within.
  #
  # Config path can be fetched using the constant org.apache.storm.Config.TOPOLOGY_WORKERS
  workers = 1

  # The maximum amount of time given to the topology to fully process a message
  # emitted by a spout. If the message is not acked within this time frame, Storm
  # will fail the message on the spout. Some spouts implementations will then replay
  # the message at a later time.
  #
  # Config path can be fetched using the constant org.apache.storm.Config.TOPOLOGY_MESSAGE_TIMEOUT_SECS
  message.timeout.secs = 30

  # When set to true, tells Storm to log every message every emitted by a component.
  # This is useful in local mode when testing topologies, but you probably want to keep this turned off
  # when running topologies on the cluster.
  #
  # Config path can be fetched using the constant org.apache.storm.Config.TOPOLOGY_DEBUG
  debug = true

  # How many instances to create for a spout/bolt. A task runs on a thread with zero or more
  # other tasks for the same spout/bolt. The number of tasks for a spout/bolt is always
  # the same throughout the lifetime of a topology, but the number of executors (threads) for
  # a spout/bolt can change over time. This allows a topology to scale to more or less resources
  # without redeploying the topology or violating the constraints of Storm (such as a fields grouping
  # guaranteeing that the same value goes to the same task).
  #
  # Config path can be fetched using the constant org.apache.storm.Config.TOPOLOGY_TASKS
  tasks = 1

  # Bolt-specific configuration for windowed bolts to specify the window length in time duration.
  #
  # Config path can be fetched using the constant org.apache.storm.Config.TOPOLOGY_BOLTS_WINDOW_LENGTH_DURATION_MS
  bolts.window.length.duration.ms = 5000

  # The maximum parallelism allowed for a component in this topology. This configuration is
  # typically used in testing to limit the number of threads spawned in local mode.
  #
  # Config path can be fetched using the constant org.apache.storm.Config.TOPOLOGY_MAX_TASK_PARALLELISM
  #max.task.parallelism = 2
}


storm {

  # A list of hosts of ZooKeeper servers used to manage the cluster.
  #
  # Config path can be fetched using the constant org.apache.storm.Config.STORM_ZOOKEEPER_SERVERS
  zookeeper.servers = "localhost"

  # The root location at which Storm stores data in ZooKeeper.
  #
  # Config path can be fetched using the constant org.apache.storm.Config.STORM_ZOOKEEPER_ROOT
  zookeeper.root = "/storm"
}


nifi {

  # NiFi URL
  #
  # Config path can be fetched using the constant TruckingTopology.NiFiUrl
  url = "http://sandbox.hortonworks.com:9090/nifi"

  truck-events {
    # NiFi output port name
    #
    # Config path can be fetched using the constant TruckingTopology.NiFiOutputPortName
    port-name = "Truck Events"

    # NiFi output batch size
    #
    # Config path can be fetched using the constant TruckingTopology.NiFiOutputBatchSize
    batch-size = 1
  }

  traffic-data {
    # NiFi output port name
    #
    # Config path can be fetched using the constant TruckingTopology.NiFiOutputPortName
    port-name = "Traffic Data"

    # NiFi output batch size
    #
    # Config path can be fetched using the constant TruckingTopology.NiFiOutputBatchSize
    batch-size = 1
  }

  input {
    # NiFi input port name
    #
    # Config path can be fetched using the constant TruckingTopology.NiFiInputPortName
    port-name = "Truck And Traffic Data"

    # NiFi input batch size
    #
    # Config path can be fetched using the constant TruckingTopology.NiFiInputBatchSize
    batch-size = 1

    # Tick frequency, in seconds
    #
    # Config path can be fetched using the constant TruckingTopology.NiFiInputTickFrequency
    tick-frequency = 5
  }
}


kafka {

  consumer {

    # A unique string that identifies the consumer group this consumer belongs to.
    # This property is required if the consumer uses either the group management functionality by
    # using subscribe(topic) or the Kafka-based offset management strategy.
    #
    # More info at: http://kafka.apache.org/documentation.html#newconsumerconfigs
    #
    # Config path can be fetched using the constant TruckingTopology.ConsumerGroupId
    group.id = "d"

    # Name of the Kafka topic to subscribe to for consuming trucking events.
    #
    # Config path can be fetched using the constant TruckingTopology.TruckingTopic
    trucking.topic = "trucking.events"
  }

  producer {

    // TODO: fix kafka comments (below and above)

    # Name of the Kafka topic to subscribe to for consuming trucking events.
    #
    # Config path can be fetched using the constant TruckingTopology.TruckingTopic
    bootstrap-servers = "localhost:9092,localhost:9092"

    # Name of the Kafka topic to subscribe to for consuming trucking events.
    #
    # Config path can be fetched using the constant TruckingTopology.TruckingTopic
    key-serializer = "org.apache.kafka.common.serialization.StringSerializer"

    # Name of the Kafka topic to subscribe to for consuming trucking events.
    #
    # Config path can be fetched using the constant TruckingTopology.EventKeyField
    value-serializer = "org.apache.kafka.common.serialization.StringSerializer"

    # Name of the Kafka topic to publish trucking events to.
    #
    # Config path can be fetched using the constant TruckingTopology.TruckingTopic
    trucking.topic = "trucking.proccessed.events"
  }
}


hbase {

  #
  #
  # Config path can be fetched using the constant TruckingTopology.EventKeyField
  event-key-field = "eventKey"

  # The HBase column family.
  #
  # Config path can be fetched using the constant TruckingTopology.ColumnFamily
  column-family = "cf"

  # Name of the HBase table storing all trucking events.
  #
  # Config path can be fetched using the constant TruckingTopology.AllTruckingEvents
  all-trucking.table = "all_trucking_events"

  # Name of the HBase table storing anomalous truck trucking events.
  #
  # Config path can be fetched using the constant TruckingTopology.AnomalousTruckingEvents
  anomalous-trucking.table = "anomalous_trucking_events"

  # Name of the HBase table storing anomalous trucking events.
  #
  # Config path can be fetched using the constant TruckingTopology.AnomalousTruckingEventsCount
  anomalous-trucking-count.table = "anomalous_trucking_event_count"
}
